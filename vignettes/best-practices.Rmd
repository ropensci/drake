---
title: "General best practices for drake projects"
author: "Will Landau"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{best-practices}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r cautionstart, echo = F}
suppressMessages(suppressWarnings(library(drake)))
suppressMessages(suppressWarnings(library(magrittr)))
suppressMessages(suppressWarnings(library(curl)))
suppressMessages(suppressWarnings(library(httr)))
suppressMessages(suppressWarnings(library(R.utils)))
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
knitr::opts_chunk$set(
  collapse = TRUE,
  error = TRUE,
  warning = TRUE
)
tmp <- file.create("data.csv")
```

This vignette describes general best practices for creating, configuring, and running `drake` projects. It answers frequently asked questions and clears up common misconceptions, and it will continuously develop in response to community feedback.

# How to organize your files

## Examples

For examples of how to structure your code files, see the beginner oriented example projects:

- [basic](https://github.com/ropensci/drake/tree/master/inst/examples/basic)
- [gsp](https://github.com/ropensci/drake/tree/master/inst/examples/gsp)
- [packages](https://github.com/ropensci/drake/tree/master/inst/examples/packages)

Write the code directly with the `drake_example()` function.

```{r exampledrakewritingbestpractices, eval = FALSE}
drake_example("basic")
drake_example("gsp")
drake_example("packages")
``` 

In practice, you do not need to organize your files the way the examples do, but it does happen to be a reasonable way of doing things.

## Where do you put your code?

It is best to write your code as a bunch of functions. You can save those functions in R scripts and then `source()` them before doing anything else.

```{r sourcefunctions, eval = FALSE}
# Load functions get_data(), analyze_data, and summarize_results()
source("my_functions.R")
```

Then, set up your workflow plan data frame.

```{r storecode1}
good_plan <- drake_plan(
  my_data = get_data(file_in("data.csv")), # External files need to be in commands explicitly. # nolint
  my_analysis = analyze_data(my_data),
  my_summaries = summarize_results(my_data, my_analysis)
)

good_plan
```

`Drake` knows that `my_analysis` depends on `my_data` because `my_data` is an argument to `analyze_data()`, which is part of the command for `my_analysis`.

```{r visgood, eval = FALSE}
config <- drake_config(good_plan)
vis_drake_graph(config)
```

<iframe
src = "https://cdn.rawgit.com/ropensci/drake/0fdb7d29/images/good-commands.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>

Now, you can call `make()` to build the targets.

```{r makestorecode, eval = FALSE}
make(good_plan)
```

If your commands are really long, just put them in larger functions. `Drake` analyzes imported functions for non-file dependencies.

## Remember: your commands are code chunks, not R scripts

Some people are accustomed to dividing their work into R scripts and then calling `source()` to run each step of the analysis. For example you might have the following files.

- `get_data.R`
- `analyze_data.R`
- `summarize_results.R`

If you migrate to `drake`, you may be tempted to set up a workflow plan like this.

```{r badsource}
bad_plan <- drake_plan(
  my_data = source(file_in("get_data.R")),
  my_analysis = source(file_in("analyze_data.R")),
  my_summaries = source(file_in("summarize_data.R"))
)

bad_plan
```

But now, the dependency structure of your work is broken. Your R script files are dependencies, but since `my_data` is not mentioned in a function or command, `drake` does not know that `my_analysis` depends on it.

```{r visbad, eval = FALSE}
config <- drake_config(bad_plan)
vis_drake_graph(config)
```

<iframe
src = "https://cdn.rawgit.com/ropensci/drake/0fdb7d29/images/bad-commands.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>

Dangers:

1. In the first `make(bad_plan, jobs = 2)`, `drake` will try to build `my_data` and `my_analysis` at the same time even though `my_data` must finish before `my_analysis` begins.
2. `Drake` is oblivious to `data.csv` since it is not explicitly mentioned in a workflow plan command. So when `data.csv` changes, `make(bad_plan)` will not rebuild `my_data`.
3. `my_analysis` will not update when `my_data` changes.
4. The return value of `source()` is formatted counter-intuitively. If `source(file_in("get_data.R"))` is the command for `my_data`, then `my_data` will always be a list with elements `"value"` and `"visible"`. In other words, `source(file_in("get_data.R"))$value` is really what you would want.

In addition, this `source()`-based approach is simply inconvenient. `Drake` rebuilds `my_data` every time `get_data.R` changes, even when those changes are just extra comments or blank lines. On the other hand, in the previous plan that uses `my_data = get_data()`, `drake` does not trigger rebuilds when comments or whitespace in `get_data()` are modified. `Drake` is R-focused, not file-focused. If you embrace this viewpoint, your work will be easier.

## R Markdown and knitr reports

For a serious project, you should use `drake`'s `make()` function outside `knitr`. In other words, you should treat R Markdown reports and other `knitr` documents as targets and imports, not as a way to run `make()`. Viewed as targets, `drake` makes special exceptions for R Markdown reports and other [knitr](https://github.com/yihui/knitr) reports such as `*.Rmd` and `*.Rnw` files. Not every `drake` project needs them, but it is good practice to use them to summarize the final results of a project once all the other targets have already been built. The basic example, for instance, has an R Markdown report. `report.Rmd` is knitted to build `report.md`, which summarizes the final results.

```{r revisitbasic}
# Load all the functions and the workflow plan data frame, my_plan.
load_basic_example() # Get the code with drake_example("basic").
```

To see where `report.md` will be built, look to the right of the workflow graph.

```{r revisitbasicgraph, eval = FALSE}
config <- drake_config(my_plan)
vis_drake_graph(config)
```

<iframe
src = "https://cdn.rawgit.com/ropensci/drake/0fdb7d29/images/outdated.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>

`Drake` treats [knitr](https://github.com/yihui/knitr) report as a special cases. Whenever `drake` sees `knit()` or `render()` ([rmarkdown](https://github.com/rstudio/rmarkdown)) mentioned in a command, it dives into the source file to look for dependencies. Consider `report.Rmd`, which you can view [here](https://github.com/ropensci/drake/blob/master/inst/examples/basic/report.Rmd). When `drake` sees `readd(small)` in an active code chunk, it knows [report.Rmd](https://github.com/ropensci/drake/blob/master/inst/examples/basic/report.Rmd) depends on the target called `small`, and it draws the appropriate arrow in the workflow graph above. And if `small` ever changes, `make(my_plan)` will re-process [report.Rmd](https://github.com/ropensci/drake/blob/master/inst/examples/basic/report.Rmd) to produce the target file `report.md`.

[knitr](https://github.com/yihui/knitr) reports are the only kind of file that `drake` analyzes for dependencies. It does not give R scripts the same special treatment.

## Workflows as R packages

The R package structure is a great way to organize the files of your project. Writing your own package to contain your data science workflow is a good idea, but you will need to use `expose_imports()` to run it properly with `drake`. Thanks to [Jasper Clarkberg](https://github.com/dapperjapper) for the workaround.

### Advantages of putting workflows in R packages

- The file organization of R packages is a well-understood community standard. If you follow it, your work may be more readable and thus reproducible.
- R package installation is a standard process. The system makes it easier for others to obtain and run your code.
- You get development and quality control tools for free: [helpers for loading code and creating files](https://github.com/hadley/devtools), [unit testing](http://r-pkgs.had.co.nz/tests.html), [package checks](http://r-pkgs.had.co.nz/check.html), [code coverage](https://github.com/r-lib/covr), and [continuous integration](https://ipub.com/continuous-integration-for-r/).

### The problem

For `drake`, there is one problem: nested functions. `Drake` always looks for imported functions nested in other imported functions, but only in your environment. When it sees a function from a package, it does not look in its body for other imports.

To see this, consider the `digest()` function from the [`digest` package](https://github.com/eddelbuettel/digest). [`Digest` package](https://github.com/eddelbuettel/digest) is a utility for computing hashes, not a data science workflow, but I will use it to demonstrate how `drake` treats imports from packages.


```{r nestingproblem}
library(digest)
g <- function(x){
  digest(x)
}
f <- function(x){
  g(x)
}
plan <- drake_plan(x = f(1))

# Here are the reproducibly tracked objects in the workflow.
tracked(plan)

# But the `digest()` function has dependencies too.
# Because `drake` knows `digest()` is from a package,
# it ignores these dependencies by default.
head(deps(digest), 10)
```

### The solution

To force `drake` to dive deeper into the nested functions in a package, you must use `expose_imports()`. Again, I demonstrate with the [`digest` package](https://github.com/eddelbuettel/digest) package, but you should really only do this with a package you write yourself to contain your workflow. For external packages, [packrat](https://rstudio.github.io/packrat/) is a much better solution for package reproducibility.

```{r nestingsolution}
expose_imports(digest)
new_objects <- tracked(plan)
head(new_objects, 10)
length(new_objects)

# Now when you call `make()`, `drake` will dive into `digest`
# to import dependencies.

cache <- storr::storr_environment() # just for examples
make(plan, cache = cache)
head(cached(cache = cache), 10)
length(cached(cache = cache))
```

```{r rmfiles_caution, echo = FALSE}
clean(destroy = TRUE, verbose = FALSE)
file.remove("report.Rmd")
unlink(
  c(
    "data.csv", "Makefile", "report.Rmd",
    "shell.sh", "STDIN.o*", "Thumbs.db"
  )
)
```

# Generating workflow plan data frames

`Drake` has the following functions to generate workflow plan data frames (the `plan` argument of `make()`, where you list your targets and commands). 

- `drake_plan()`
- `evaluate_plan()`
- `expand_plan()`
- `gather_plan()`
- `plan_analyses()`
- `plan_summaries()`

Except for `drake_plan()`, they all use wildcards as templates. For example, suppose your workflow checks several metrics of several schools. The idea is to write a workflow plan with your metrics and let the wildcard templating expand over the available schools.

```{r schoolswildcards1}
hard_plan <- drake_plan(
  credits = check_credit_hours(school__),
  students = check_students(school__),
  grads = check_graduations(school__),
  public_funds = check_public_funding(school__)
)

evaluate_plan(
  hard_plan, 
  rules = list(school__ = c("schoolA", "schoolB", "schoolC"))
) 
```

But what if some metrics do not make sense? For example, what if `schoolC` is a completely privately-funded school? With no public funds, `check_public_funds(schoolC)` may quit in error if we are not careful. This is where setting up workflow plans gets tricky. You may need to use multiple wildcards and make some combinations of values are left out.

```{r rulesgridschools}
library(magrittr)
rules_grid <- tibble::tibble(
  school_ =  c("schoolA", "schoolB", "schoolC"),
  funding_ = c("public", "public", "private"),
) %>% 
  tidyr::crossing(cohort_ = c("2012", "2013", "2014", "2015")) %>%
  dplyr::filter(!(school_ == "schoolB" & cohort_ %in% c("2012", "2013"))) %>%
  print()
```

Then, alternately choose `expand = TRUE` and `expand = FALSE` when evaluating the wildcards.

```{r rulesgridevalplan}
drake_plan(
  credits = check_credit_hours("school_", "funding_", "cohort_"),
  students = check_students("school_", "funding_", "cohort_"),
  grads = check_graduations("school_", "funding_", "cohort_"),
  public_funds = check_public_funding("school_", "funding_", "cohort_"),
  strings_in_dots = "literals"
) %>% evaluate_plan(
    wildcard = "school_",
    values = rules_grid$school_,
    expand = TRUE
  ) %>%
  evaluate_plan(
    wildcard = "funding_",
    rules = rules_grid,
    expand = FALSE
  ) %>%
  DT::datatable()
```

Thanks to [Alex Axthelm](https://github.com/AlexAxthelm) for this example in [issue 235](https://github.com/ropensci/drake/issues/235).

# Remote data sources

Some workflows rely on remote data from the internet, and the workflow needs to refresh when the datasets change. As an example, let us consider the download logs of [CRAN packages](https://cran.r-project.org/).

```{r logs1}
library(drake)
library(R.utils) # For unzipping the files we download.
library(curl)    # For downloading data.
library(httr)    # For querying websites.

url <- "http://cran-logs.rstudio.com/2018/2018-02-09-r.csv.gz"
```

How do we know when the data at the URL changed? We get the time that the file was last modified. (Alternatively, we could use an HTTP ETag.)

```{r logs2}
query <- HEAD(url)
timestamp <- query$headers[["last-modified"]]
timestamp
```

In our workflow plan, the timestamp is a target and a dependency. When the timestamp changes, so does everything downstream.

```{r logs3}
cranlogs_plan <- drake_plan(
  timestamp = HEAD(url)$headers[["last-modified"]],
  logs = get_logs(url, timestamp),
  strings_in_dots = "literals"
)
cranlogs_plan
```

To make sure we always have the latest timestamp, we use the `"always"` trigger. (See [this section of the debugging vignette](https://github.com/ropensci/drake/blob/master/vignettes/debug.Rmd#test-with-triggers) for more on triggers.)

```{r logs4}
cranlogs_plan$trigger = c("always", "any")
cranlogs_plan
```

Lastly, we define the `get_logs()` function, which actually downloads the data.

```{r logs5}
# The ... is just so we can write dependencies as function arguments
# in the workflow plan.
get_logs <- function(url, ...){
  curl_download(url, "logs.csv.gz")       # Get a big file.
  gunzip("logs.csv.gz", overwrite = TRUE) # Unzip it.
  out <- read.csv("logs.csv", nrows = 4)  # Extract the data you need.
  unlink(c("logs.csv.gz", "logs.csv"))    # Remove the big files
  out                                     # Value of the target.
}
```

When we are ready, we run the workflow.

```{r logs6}
make(cranlogs_plan)

readd(logs)
```

```{r endofline_bestpractices, echo = F}
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
```
