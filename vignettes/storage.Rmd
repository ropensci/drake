---
title: "Storage"
subtitle: "Caching, hashing, and customization"
author: "Will Landau"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{storage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r suppression, echo = F}
suppressMessages(suppressWarnings(library(drake)))
clean(destroy = TRUE, verbose = FALSE)
unlink(
  c(
    "Makefile", "report.Rmd", "shell.sh",
    "STDIN.o*", "Thumbs.db",
    "faster_cache", "my_storr"
  ),
  recursive = TRUE
)
knitr::opts_chunk$set(
  collapse = TRUE,
  error = TRUE,
  warning = TRUE
)
```

`Drake`'s `make()` function generates your project's output, and `drake` takes storing this output seriously. This guide explains how `drake` caches and hashes its data, and describes customization options that can increase convenience and speed.

# Storage basics

When you run `make()`, `drake` stores your imports and output targets in a hidden cache.


```{r mtcars_storage}
library(drake)
load_mtcars_example(verbose = FALSE) # Get the code with drake_example("mtcars").
config <- make(my_plan, verbose = FALSE)
```

You can explore your cached data using functions like `loadd()`, `readd()`, and `cached()`.

```{r explore_mtcars}
head(cached())

head(readd(small))

loadd(large)

head(large)

rm(large) # Does not remove `large` from the cache.
```

# Caches as R objects

The [storr](https://github.com/richfitz/storr) package does the heavy lifting. A `storr` is an object in R that serves as an abstraction for a storage backend, usually a file system. See the [main storr vignette](https://cran.r-project.org/package=storr/vignettes/storr.html) for a thorough walkthrough.

```{r get_storrs}
class(config$cache) # from `config <- make(...)`

cache <- get_cache() # Get the default cache from the last build.

class(cache)

cache$list() # Functionality from storr

head(cache$get("small")) # Functionality from storr
```

# Hash algorithms

The concept of [hashing](https://en.wikipedia.org/wiki/Hash_function) is central to [storr](https://github.com/richfitz/storr)'s internals. [Storr](https://github.com/richfitz/storr) uses hashes to label stored objects, and `drake` leverages these hashes to figure out which targets are up to date and which ones are outdated. A hash is like a target's fingerprint, so the hash changes when the target changes. Regardless of the target's size, the hash is always the same number of characters.

```{r hashes}
library(digest) # package for hashing objects and files
smaller_data <- 12
larger_data <- rnorm(1000)

digest(smaller_data) # compute the hash

digest(larger_data)
```

However, different hash algorithms vary in length.

```{r compare_algo_lengths}
digest(larger_data, algo = "sha512")

digest(larger_data, algo = "md5")

digest(larger_data, algo = "xxhash64")

digest(larger_data, algo = "murmur32")
```

# Which hash algorithm should you choose?

Hashing is expensive, and unsurprisingly, shorter hashes are usually faster to compute. So why not always use `murmur32`? One reason is the risk of collisions: that is, when two different objects have the same hash. In general, shorter hashes have more frequent collisions. On the other hand, a longer hash is not always the answer. Besides the loss of speed, `drake` and [storr](https://github.com/richfitz/storr) sometimes use hash keys as file names, and long hashes could violate the 260-character cap on Windows file paths. That is why `drake` uses a shorter hash algorithm for internal cache-related file names and a longer hash algorithm for everything else.

```{r justified_hash_choices}
default_short_hash_algo()

default_long_hash_algo()

short_hash(cache)

long_hash(cache)
```

# Select the hash algorithms of the default cache

For new projects, use `new_cache()` to set the hash algorithms of the default cache.

```{r default_cache_reset}
# cache_path(cache) # Default cache from before. # nolint

# Start from scratch to reset both hash algorithms.
clean(destroy = TRUE)

tmp <- new_cache(
  path = default_cache_path(), # The `.drake/` folder.
  short_hash_algo = "crc32",
  long_hash_algo = "sha1"
)
```

The cache at `default_cache_path()` (equivalently, the `.drake/` folder) is the default cache used for `make()`.

```{r default_cache_control}
config <- make(my_plan, verbose = FALSE)

short_hash(config$cache) # xxhash64 is the default_short_hash_algo()

long_hash(config$cache) # sha256 is the default_long_hash_algo()
```

You can change the long hash algorithm without throwing away the cache, but your project will rebuild from scratch. As for the short hash, you are committed until you delete the cache and all its supporting files.

```{r more_cache}
outdated(config) # empty

config$cache <- configure_cache(
  config$cache,
  long_hash_algo = "murmur32",
  overwrite_hash_algos = TRUE
)
```

Below, the targets become outdated because the existing hash keys do not match the new hash algorithm.

```{r newhashmorecache}
config <- drake_config(my_plan, verbose = FALSE, cache = config$cache)
outdated(config)

config <- make(my_plan, verbose = FALSE)

short_hash(config$cache) # same as before

long_hash(config$cache) # different from before
```


# More on custom caches

You do not need to use the default cache at the `default_cache_path()` (`.drake/`). However, if you use a different file system, such as the custom `faster_cache/` folder below, you will need to manually supply the cache to all functions that require one.

```{r, custom cache}
faster_cache <- new_cache(
  path = "faster_cache",
  short_hash_algo = "murmur32",
  long_hash_algo = "murmur32"
)

# cache_path(faster_cache) # nolint
# cache_path(cache) # location of the previous cache # nolint

short_hash(faster_cache)

long_hash(faster_cache)

new_plan <- drake_plan(
  simple = 1 + 1
)

make(new_plan, cache = faster_cache)

cached(cache = faster_cache)

readd(simple, cache = faster_cache)
```

# Recovering the cache

You can recover an old cache from the file system. You could use `storr::storr_rds()` directly if you know the short hash algorithm, but `this_cache()` and `recover_cache()` are safer for `drake`. `get_cache()` is similar, but it has a slightly different interface.

```{r oldcachenoeval, eval = FALSE}
old_cache <- this_cache("faste_cache") # Get a cache you know exists...
recovered <- recover_cache("faster_cache") # or create a new one if missing.
```

# Custom [storr](https://github.com/richfitz/storr) caches

If you want bypass `drake` and generate a cache directly from [storr](https://github.com/richfitz/storr), it is best to do so right from the beginning.

```{r use_storr_directly}
library(storr)
my_storr <- storr_rds("my_storr", mangle_key = TRUE)
make(new_plan, cache = my_storr)

cached(cache = my_storr)

readd(simple, cache = my_storr)
```

In addition to `storr_rds()`, `drake` supports in-memory caches created from `storr_environment()`. However, parallel computing is not supported these caches. The `jobs` argument must be 1, and the `parallelism` argument must be either `"mclapply"` or `"parLapply"`. (It is sufficient to leave the default values alone.)

```{r memory_caches}
memory_cache <- storr_environment()
other_plan <- drake_plan(
  some_data = rnorm(50),
  more_data = rpois(75, lambda = 10),
  result = mean(c(some_data, more_data))
)

make(other_plan, cache = memory_cache)

cached(cache = memory_cache)

readd(result, cache = memory_cache)
```

In theory, it should be possible to leverage serious databases using `storr_dbi()`. However, if you use such caches, please heed the following.

1. Be sure you have [storr](https://github.com/richfitz/storr) version 1.1.3 or greater installed.
1. Be careful about parallel computing. For example the `storr::storr_dbi()` cache is not thread-safe. Either use no parallel computing at all or set `parallelism = "future"` with `caching = "master"`. The `"future"` backend is currently experimental, but it allows the master process to do all the caching in order to avoid race conditions.

The following example requires the `DBI` and `RSQLite` packages.

```{r dbi_caches, eval = FALSE}
mydb <- DBI::dbConnect(RSQLite::SQLite(), "my-db.sqlite")
cache <- storr::storr_dbi(
  tbl_data = "data",
  tbl_keys = "keys",
  con = mydb
)
load_mtcars_example() # Get the code with drake_example("mtcars").
unlink(".drake", recursive = TRUE)
make(my_plan, cache = cache)
```

# Cleaning up

If you want to start from scratch, you can `clean()` the cache. Use the `destroy` argument to remove it completely. `cache$del()` and `cache$destroy()` are also options, but they leave output file targets dangling. By contrast, `clean(destroy = TRUE)` removes file targets generated by `drake::make()`. `drake_gc()` and `clean(..., garbage_collection = TRUE)` do garbage collection, and `clean(purge = TRUE)` removes all target-level data, not just the final output values.

```{r cleaning_up}
clean(small, large)

cached() # 'small' and 'large' are gone

clean(destroy = TRUE)

clean(destroy = TRUE, cache = faster_cache)
clean(destroy = TRUE, cache = my_storr)
```

```{r cleanup_storage, echo = FALSE}
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
```
