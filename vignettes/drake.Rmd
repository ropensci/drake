---
title: "drake"
subtitle: "data frames in R for Make"
author: "William Michael Landau"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{drake}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = F}
suppressMessages(suppressWarnings(library(drake)))
suppressMessages(suppressWarnings(library(magrittr)))
unlink(".drake", recursive = TRUE)
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
knitr::opts_chunk$set(
  collapse = TRUE,
  error = TRUE,
  warning = TRUE
)
```

Reproducibility carries an implicit promise that alleged results of a computation are up to date with the dependencies. In other words, the final tables, figures, and reports should match the data and code that generated them. This form of internal consistency is nontrivial for medium to large projects, where dependencies change frequently but the results take a long time to refresh. In fact, dependency awareness is arguably just as important as [scientific replicability](http://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970), [literate programming](https://www.r-bloggers.com/reproducible-research-training-wheels-and-knitr/), and [version control](https://nicercode.github.io/git/why.html). However, few tools in R properly address the need, and it is a conspicuous blind spot in most discussions about reproducibility in the R and Statistics communities.

# Reproducibility with drake

![](logo-vignettes.png)

Enter the [`drake` package](https://github.com/wlandau-lilly/drake), one of the only [pipeline toolkits](https://github.com/pditommaso/awesome-pipeline) available in R. `Drake` detects the dependencies of intermediate steps, links everything together in a [network graph](https://cran.r-project.org/package=drake/vignettes/graph.html#dependency-reactivity), fingerprints the available data, and rebuilds only the pieces that are out of date or missing. For a demonstration, we turn to the built-in basic example.

```{r reproducibilitydemo}
load_basic_example()
```

The intermediate components of the project, or "targets", are declared in an ordinary data frame.

```{r myplandrakevig}
my_plan
```

There are [several functions](https://github.com/wlandau-lilly/drake/blob/master/README.md#useful-functions) to help you generate the workflow plan. No need to write every row by hand.

```{r workplangeneration}
library(magrittr)
dataset_plan <- workplan(
  small = simulate(5),
  large = simulate(50)
)
dataset_plan

analysis_methods <- workplan(
  regression = regNUMBER(dataset__)
) %>%
  evaluate_plan(wildcard = "NUMBER", values = 1:2)
analysis_methods

analysis_plan <- plan_analyses(
  plan = analysis_methods,
  datasets = dataset_plan
)
analysis_plan

whole_plan <- rbind(dataset_plan, analysis_plan)
whole_plan
```

The [visNetwork graph](http://datastorm-open.github.io/visNetwork/) below is interactive, so you can hover, click, drag, zoom, and pan.

```{r drakevisgraph, eval = FALSE}
vis_drake_graph(my_plan)
```

<iframe
src = "https://cdn.rawgit.com/wlandau-lilly/drake/bd8a086f/images/outdated.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>

All the targets are out of date because nothing has been built yet.

```{r outdateddrake}
config <- drake_config(my_plan, verbose = FALSE) # Master configuration list
outdated(config)
```

To build the project, `make()` executes the commands from the workflow plan in the [correct order](https://en.wikipedia.org/wiki/Topological_sorting).

```{r firstmakedrake}
make(my_plan)
```

Now, everything is up to date. In the next `make()`, there is no work to do.

```{r makeuptodatedrake}
make(my_plan)
```

If you nontrivially change a command, imported function, or other dependency, `make()` does the minimum amount of work necessary to bring your project up to date again. Below, `make()` skips the targets that depend on `reg1()`.

```{r reg2makedrake}
reg2 <- function(d){
  d$x3 <- d$x ^ 3
  lm(y ~ x3, data = d)
}
make(my_plan)
```

# High-performance computing

In addition to reproducibility, `drake` tries to make high-performance computing easy and accessible. Parallel processing is as straightforward as setting the maximum number of simultaneous parallel workers, or `jobs`.

```{r jobsdrake, eval = FALSE}
make(my_plan, jobs = 2)
```

`Drake` can even recommend the maximum number of useful jobs. The `max_useful_jobs()` function takes into account which targets will be skipped in the next `make()`.

```{r maxusefuljobs}
# Everything is already up to date.
config <- drake_config(my_plan, verbose = FALSE)
max_useful_jobs(config)

reg2 <- function(d){
  d$x4 <- d$x ^ 4
  lm(y ~ x4, data = d)
}

# The targets that depend on reg2() are now outdated.
max_useful_jobs(config)
```

To make its recommendation, `max_useful_jobs()`, relies on `drake`'s approach to [implicit parallelism](https://www.computerhope.com/jargon/i/implicit-parallelism.htm).

```{r reg2graphdrake, eval = FALSE}
vis_drake_graph(config)
```

<iframe
src = "https://cdn.rawgit.com/wlandau-lilly/drake/bd8a086f/images/reg2.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>

The columns in the graph represent parallelizable stages. All the objects in a column are conditionally independent given their dependencies. In `make()`, the columns are processed sequentially from left to right, and everything within an individual column is parallelized over the available `jobs`. (This is not a hard and fast rule: `drake` includes targets further ahead if some targets in the current column are already up to date.) Assuming the targets are rate-limiting (as opposed to the imports), the next `make(..., jobs = 4)` should be faster than `make(..., jobs = 1)`, and it would be superfluous to use more than 4 `jobs`.

`Drake` supports a [vast arsenal](https://github.com/wlandau-lilly/drake/blob/master/vignettes/parallelism.Rmd#parallel-backends) of parallel computing backends. You can leverage the spare cores on your laptop's processor or scale up to serious distributed computing on a cluster. You can list the supported backends with `parallellism_choices()`.

```{r parallelchoicesdrake}
parallelism_choices()
```

and make use of a backend with the `parallelism` argument.

```{r declareparalleldrake, eval = FALSE}
make(my_plan, parallelism = "mclapply", jobs = 2)
```

You can configure the `"future_lapply"` and `"Makefile"` backends for formal job schedulers such as [SLURM](https://slurm.schedmd.com/), [TORQUE](www.adaptivecomputing.com/products/open-source/torque/), and the [Sun/Univa Grid Engine](http://www.univa.com/products/). For `"future_lapply"` parallelism, use the backend's `workers` argument for the targets and `make()`'s `jobs` argument for the imports. See the [parallelism vignette](https://github.com/wlandau-lilly/drake/blob/master/vignettes/parallelism.Rmd) for more detailed instructions.

```{r exampleconfiguredrake, eval = FALSE}
library(future.batchtools)
future::plan(
  batchtools_slurm(
    template = "batchtools.slurm.tmpl",
    workers = 8
  )
)
make(my_plan, parallelism = "future_lapply", jobs = 4)
```

With the `drake_example()` function, you can write [example files](https://github.com/wlandau-lilly/drake/tree/master/inst/examples) to get you started. Most of the examples focus on the available high-performance computing options. See `drake_examples()` for your choices.

```{r drakeexamplesdrake}
drake_examples()
```

# Acknowledgements and related work

The original idea of a time-saving reproducible build system extends back at least as far as [GNU Make](http://kbroman.org/minimal_make/), which still aids the work of [data scientists](http://blog.kaggle.com/2012/10/15/make-for-data-scientists/) as well as the original user base of complied language programmers. In fact, the name "drake" stands for "Data Frames in R for Make".

Today, there is a [whole ecosystem of pipeline toolkits](https://github.com/pditommaso/awesome-pipeline), mostly written in Python. Of all the toolkits in the list, [Rich FitzJohn](http://richfitz.github.io/)'s [remake package](https://github.com/richfitz/remake) is by far the most important for `drake`. `Drake` stands squarely on the shoulders of [remake](https://github.com/richfitz/remake), borrowing the fundamental concepts and extending them in a fresh implementation with a convenient interface and high-performance computing.

Many thanks to the following people for contributing amazing ideas and code patches early in the development of `drake` and its predecessors [parallelRemake](https://github.com/wlandau/parallelRemake) and [remakeGenerator](https://github.com/wlandau/remakeGenerator).

- [Alex Axthelm](https://github.com/AlexAxthelm)
- [Chan-Yub Park](https://github.com/mrchypark)
- [Daniel Falster](https://github.com/dfalster)
- [Eric Nantz](https://github.com/enantz-lilly)
- [Henrik Bengtsson](https://github.com/HenrikBengtsson)
- [Jarad Niemi](http://www.jarad.me/)
- [Jasper Clarkberg](https://github.com/dapperjapper)
- [Kendon Bell](https://github.com/kendonB)
- [Kirill M&uuml;ller](https://github.com/krlmlr)

Special thanks to [Jarad Niemi](http://www.jarad.me/), my advisor from [graduate school](http://stat.iastate.edu/), for first introducing me to the idea of [Makefiles](https://www.gnu.org/software/make/) for research. It took several months to convince me, and I am glad he succeeded.
/github.com/pditommaso/awesome-pipeline), but few are R-focused.

```{r rmfiles_main, echo = FALSE}
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
```
