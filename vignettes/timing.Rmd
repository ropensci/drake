---
title: "Time: logging, prediction, and strategy"
author: "Will Landau"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{timing}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r suppression_timing, echo = F}
suppressMessages(suppressWarnings(library(drake)))
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
knitr::opts_chunk$set(
  collapse = TRUE,
  error = TRUE,
  warning = TRUE,
  fig.width = 6,
  fig.height = 6,
  fig.align = "center"
)
```

Thanks to [Jasper Clarkberg](https://github.com/dapperjapper), `drake` records how long it takes to build each target. For large projects that take hours or days to run, this feature becomes important for planning and execution.

```{r timing_intro}
library(drake)
load_mtcars_example() # Get the code with drake_example("mtcars").
make(my_plan, jobs = 2)

build_times(digits = 8) # From the cache.

# `dplyr`-style `tidyselect` commands
build_times(starts_with("coef"), digits = 8)

build_times(digits = 8, targets_only = TRUE)
```

For `drake` version 4.1.0 and earlier, `build_times()` just measures the elapsed runtime of each command in `my_plan$command`. For later versions, the build times also account for all the internal operations in `drake:::build()`, such as [storage and hashing](https://github.com/ropensci/drake/blob/master/vignettes/storage.Rmd).

# Predict total runtime

Drake uses these times to predict the runtime of the next `make()`. At this moment, everything is up to date in the current example, so the next `make()` should be fast. Here, we only factor in the times of the formal targets in the workflow plan, excluding any imports.

```{r predict_runtime}
config <- drake_config(my_plan, verbose = FALSE)
predict_runtime(config, targets_only = TRUE)
```

Suppose we change a dependency to make some targets out of date. Now, even though, the next `make()` should take a little longer.

```{r changedep_timing}
reg2 <- function(d){
  d$x3 <- d$x ^ 3
  lm(y ~ x3, data = d)
}

predict_runtime(config, targets_only = TRUE)
```

But what if you plan on starting from scratch next time, either after `clean()` or with `make(..., trigger = "always")`?

```{r predict_runtime_scratch}
predict_runtime(config, from_scratch = TRUE, targets_only = TRUE)
```

# Strategize your high-performance computing

Let's say you are scaling up your workflow. You just put bigger data and heavier computation in your custom code, and the next time you run `make()`, your targets will take much longer to build. In fact, you estimate that every target except for your R Markdown report will take two hours to complete. Let's write down these known times in seconds.

```{r knowntimes}
known_times <- c(5, rep(7200, nrow(my_plan) - 1))
names(known_times) <- c(file_store("report.md"), my_plan$target[-1])
known_times
```

How many parallel jobs should you use in the next `make()`? The `predict_runtime()` function can help you decide. `predict_runtime(jobs = n)` simulates persistent parallel workers and reports the estimated total runtime of `make(jobs = n)`. (See also `predict_load_balancing()`.)

```{r predictjobs}
time <- c()
for (jobs in 1:12){
  time[jobs] <- predict_runtime(
    config,
    jobs = jobs,
    from_scratch = TRUE,
    known_times = known_times
  )
}
library(ggplot2)
ggplot(data.frame(time = time / 3600, jobs = ordered(1:12), group = 1)) +
  geom_line(aes(x = jobs, y = time, group = group)) +
  scale_y_continuous(breaks = 0:10 * 4, limits = c(0, 29)) +
  theme_gray(16) +
  xlab("jobs argument of make()") +
  ylab("Predicted runtime of make() (hours)")
```

We see considerable speed gains up to 4 jobs, but then we have to double the jobs to shave off another 2 hours. The extra time savings may be worth your while in a pinch, but otherwise 4 is a solid choice.

```{r endofline_timing, echo = F}
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
```
