---
title: "Quickstart"
subtitle: "quickstart example for drake"
author: "William Michael Landau"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{quickstart}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

![](logo-vignettes.png)

```{r suppression, echo = F}
suppressMessages(suppressWarnings(library(drake)))
clean(destroy = TRUE)
```

# Quick examples

```r
library(drake)
load_basic_example() # Also (over)writes report.Rmd. `example_drake("basic")`, `vignette("quickstart")`.
plot_graph(my_plan) # Hover, click, drag, zoom, pan. Try file = "graph.html" and targets_only = TRUE.
make(my_plan) # Run the workflow.
make(my_plan) # Check that everything is already up to date.
```

Dive deeper into the built-in examples.

```{r noeval2, eval = FALSE}
example_drake("basic") # Write the code files.
examples_drake() # List the other examples.
vignette("quickstart") # Same as https://cran.r-project.org/package=drake/vignettes/quickstart.html
```

# Useful functions

Besides `make()`, here are some useful functions to learn about drake,

```r
load_basic_example()
drake_tip()
examples_drake()
example_drake()
```

set up your workflow plan,

```r
plan()
analyses()
summaries()
evaluate()
expand()
gather()
```

explore the dependency network,
```r
outdated()
missed()
plot_graph()
dataframes_graph()
render_graph()
read_graph()
deps()
tracked()
```

interact with the cache,
```r
clean()
cached()
imported()
built()
readd()
loadd()
find_project()
find_cache()
```

debug your work,
```r
check()
session()
in_progress()
progress()
config()
read_config()
```

and speed up your project with parallel computing.

```r
make() # with jobs > 2
max_useful_jobs()
parallelism_choices()
shell_file()
```

# Setting up the basic example

Let's establish the building blocks of a data analysis workflow.

```{r libs}
library(knitr)
library(drake)
```

First, we will generate a few datasets.

```{r sim}
simulate = function(n){
  data.frame(
    x = stats::rnorm(n), # Drake tracks calls like `pkg::fn()` (namespaced functions).
    y = rpois(n, 1)
  )
}
```

Then, we will analyze each dataset with multiple analysis methods.

```{r reg}
reg1 = function(d){
  lm(y ~ + x, data = d)
}

reg2 = function(d){
  d$x2 = d$x^2
  lm(y ~ x2, data = d)
}
```

Finally, we will generate a [dynamic report](http://rmarkdown.rstudio.com/) to display results.

```{r knit}
my_knit = function(file, ...){
  knit(file)
}
```

We need the source file `report.Rmd`.

```{r file}
lines = c(
  "---",
  "title: Example Report",
  "author: You",
  "output: html_document",
  "---",
  "",
  "Look how I read outputs from the drake cache.",
  "",
  "```{r example_chunk}",
  "library(drake)",
  "readd(small)",
  "readd(coef_regression2_small)", # Return an object from the drake cache.
  "loadd(large)", # Load an object from the drake cache into your workspace.
  "head(large)",
  "```")
writeLines(lines, "report.Rmd")
```

# Workflow plan

The workflow plan lists the intermediate steps of your project.

```{r previewmyplan}
load_basic_example()
my_plan
```

Each row is an intermediate step, and each **command** generates a **target**. A target is an output R object (cached when generated) or output file (specified with single quotes), and a command just an ordinary piece of R code (not necessarily a single function call). As input, commands may take objects imported from your workspace, targets generated by other commands, or initial input files. These dependencies give your project an underlying network.

```{r plotgraph2}
# Skip the file argument to just plot.
# Hover, click, drag, zoom, pan.
plot_graph(my_plan, width = "100%", height = "500px", 
  file = "quickstart_graph1.html") 
# See also dataframes_graph(), render_graph().
# Speed up regraphing with config().
```

<iframe src="quickstart_graph1.html" width = "100%" height = "600px" allowtransparency="true" style="border: none; box-shadow: none"></iframe>


You can also check the dependencies of individual targets.

```{r checkdeps}
deps(reg2)
deps(my_plan$command[1]) # report.Rmd is single-quoted because it is a file dependency.
deps(my_plan$command[16])
```

List all the reproducibly-tracked objects and files, including imports and targets.

```{r tracked}
tracked(my_plan, targets = "small")
tracked(my_plan)
```

Check for cycles, missing input files, and other pitfalls.

```{r check}
check(my_plan)
```

# Generate the workflow plan

The data frame `my_plan` would be a pain to write by hand, so `drake` has functions to help you.

## my_plan

```{r datasets}
my_datasets = plan(
  small = simulate(5),
  large = simulate(50))
my_datasets
```

For multiple replicates:

```{r expand}
expand(my_datasets, values = c("rep1", "rep2"))
```

Each dataset is analyzed multiple ways.

```{r methods}
methods = plan(
  regression1 = reg1(..dataset..),
  regression2 = reg2(..dataset..))
methods
```

We evaluate the `..dataset..` wildcard.

```{r analyses}
my_analyses = analyses(methods, data = my_datasets)
my_analyses
```

Next, we summarize each analysis of each dataset using summary statistics and regression coefficients. 

```{r summaries}
summary_types = plan(
  summ = suppressWarnings(summary(..analysis..)), # Occasionally there is a perfect regression fit.
  coef = coef(..analysis..))
summary_types

results = summaries(summary_types, analyses = my_analyses, 
  datasets = my_datasets, gather = NULL)
results
```

The `gather` feature groups summaries into a smaller number of more manageable targets. I shut it off here to make the data frames more readable.

For the dynamic report, we have to declare the dependencies manually.

```{r reportdeps}
load_in_report = plan(
  report_dependencies = c(small, large, coef_regression2_small))
load_in_report
```

Remember: use single quotes for file dependencies. The functions `quotes()`, `unquote()`, and `strings()` from the `eply` package may help. Also, please be aware that drake cannot track entire directories/folders.

```{r reportplan}
report = plan(
  report.md = my_knit('report.Rmd', report_dependencies),
  file_targets = TRUE, strings_in_dots = "filenames")
report
```

Finally, gather your workflow together with `rbind()`. Row order does not matter.

```{r wholeplan}
my_plan = rbind(report, my_datasets, load_in_report, my_analyses, results)
my_plan
```

## Flexible helpers to make workflow plans

If your workflow does not fit the rigid datasets/analyses/summaries framework, check out functions `expand()`, `evaluate()`, and `gather()`.

```{r}
df = plan(data = simulate(center = MU, scale = SIGMA))
df
df = expand(df, values = c("rep1", "rep2"))
df
evaluate(df, wildcard = "MU", values = 1:2)
evaluate(df, wildcard = "MU", values = 1:2, expand = FALSE)
evaluate(df, rules = list(MU = 1:2, SIGMA = c(0.1, 1)), expand = FALSE)
evaluate(df, rules = list(MU = 1:2, SIGMA = c(0.1, 1, 10)))
gather(df)
gather(df, target = "my_summaries", gather = "rbind")
```

# Run the workflow

Just `make(my_plan)`.

```{r firstmake}
outdated(my_plan) # These are the targets that need to be (re)built.
missed(my_plan) # Make sure nothing is missing from your workspace.
make(my_plan)
```

The non-file dependencies of your last target are already loaded in your workspace.

```{r autoload}
"report_dependencies" %in% ls() # Should be TRUE.
```

```{r plotgraphfirstmake}
outdated(my_plan) # Everything is up to date.
plot_graph(my_plan, width = "100%", height = "500px",
  file = "quickstart_graph2.html") # The red nodes from before turned green.
# dataframes_graph(my_plan) # Get visNetwork nodes and edges so you can make your own plot.
```

<iframe src="quickstart_graph2.html" width = "100%" height = "600px" allowtransparency="true" style="border: none; box-shadow: none"></iframe>

Use `readd()` and `loadd()` to load more targets. (They are cached in the hidden `.drake/` folder using [storr](https://CRAN.R-project.org/package=storr)). Other functions interact and view the cache.

```{r cache}
readd(coef_regression2_large)
loadd(small)
head(small)
rm(small)
cached(small, large)
cached()
built()
imported()
head(read_plan())
# read_graph() # Plots the graph of the workflow you just ran.
head(progress()) # See also in_progress()
# session(): sessionInfo() of the last call to make()
progress(large)
```

The next time you run `make(my_plan)`, nothing will be built because drake knows everything is up to date.

```{r uptodateinvig}
make(my_plan)
```

But if you change one of your functions, commands, or other dependencies, drake will update the affected parts of the workflow. Let's say we want to change the quadratic term to a cubic term in our `reg2()` function.

```{r changereg2invignette}
reg2 = function(d){
  d$x3 = d$x^3
  lm(y ~ x3, data = d)
}
```

The targets depending on `reg2()` need to be rebuilt and everything else is left alone.

```{r plotwithreg2}
outdated(my_plan)
plot_graph(my_plan, width = "100%", height = "500px",
  file = "quickstart_graph3.html")
```

<iframe src="quickstart_graph3.html" width = "100%" height = "600px" allowtransparency="true" style="border: none; box-shadow: none"></iframe>

```{r remakewithreg2}
make(my_plan)
```

But trivial changes to whitespace and comments are totally ignored in your functions and in `my_plan$command`.

```{r trivial}
reg2 = function(d){
  d$x3 = d$x^3
    lm(y ~ x3, data = d) # I indented here.
}
outdated(my_plan) # Everything is up to date.
```

Need to add new work on the fly? Just append rows to the workflow plan. If the rest of your workflow is up to date, only the new work is run.

```{r newstuff}
new_simulation = function(n){
  data.frame(x = rnorm(n), y = rnorm(n))
}

additions = plan(
  new_data = new_simulation(36) + sqrt(10))  
additions

my_plan = rbind(my_plan, additions)
my_plan

make(my_plan)
```

If you ever need to erase your work, use `clean()`. Any targets removed from the cache will have to be rebuilt on the next call to `make()`, so be careful.

```{r cleanup}
clean(small, reg1) # uncaches individual targets and imported objects
clean() # cleans all targets out of the cache
clean(destroy = TRUE) # removes the cache entirely
```

# High-performance computing

The network graph is the key to drake's parallel computing.

```{r plotgraph}
clean()
load_basic_example()
make(my_plan, jobs = 2, verbose = FALSE) # Parallelize over 2 jobs.
reg2 = function(d){ # Change a dependency.
  d$x3 = d$x^3
  lm(y ~ x3, data = d)
}
plot_graph(my_plan, width = "100%", height = "500px",
  file = "quickstart_graph4.html") # Click, drag, and zoom to explore.
```

<iframe src="quickstart_graph4.html" width = "100%" height = "600px" allowtransparency="true" style="border: none; box-shadow: none"></iframe>

When you call `make(my_plan, jobs = 4)`, the work proceeds in chronological order from left to right. The items are built or imported column by column in sequence, and up-to-date targets are skipped. Within each column, the targets/objects are all independent of each other conditional on the previous steps, so they are distributed over the 4 available parallel jobs/workers. Assuming the targets are rate-limiting (as opposed to imported objects), the next `make(..., jobs = 4)` should be faster than `make(..., jobs = 1)`, but it would be superfluous to use more than 4 jobs. 

See function `max_useful_jobs()` to suggest the number of jobs, taking into account which targets are already up to date. Try out the following in a fresh R session.

```r
library(drake)
load_basic_example()
plot_graph(my_plan) # Set targets_only to TRUE for smaller graphs.
max_useful_jobs(my_plan) # 8
max_useful_jobs(my_plan, imports = "files") # 8
max_useful_jobs(my_plan, imports = "all") # 10
max_useful_jobs(my_plan, imports = "none") # 8
make(my_plan, jobs = 4)
plot_graph(my_plan)
# Ignore the targets already built.
max_useful_jobs(my_plan) # 1
max_useful_jobs(my_plan, imports = "files") # 1
max_useful_jobs(my_plan, imports = "all") # 10
max_useful_jobs(my_plan, imports = "none") # 0
# Change a function so some targets are now out of date.
reg2 = function(d){
  d$x3 = d$x^3
  lm(y ~ x3, data = d)
}
plot_graph(my_plan)
max_useful_jobs(my_plan) # 4
max_useful_jobs(my_plan, imports = "files") # 4
max_useful_jobs(my_plan, imports = "all") # 10
max_useful_jobs(my_plan, imports = "none") # 4
```



As for how the parallelism is implemented, you can choose from multiple built-in backends.

1. **mclapply**: low-overhead, light-weight. `drake::make(my_plan, parallelism = "mclapply", jobs = 2)` invokes `parallel::mclapply()` under the hood, distributing the work over at most two independent processes (set with `jobs`). Mclapply is an ideal choice for low-overhead single-node parallelism, but it does not work on Windows.
2. **parLapply**: medium-overhead, light-weight. `make(my_plan, parallelism = "parLapply", jobs = 2)` invokes `parallel::mclapply()` under the hood. This option is similar to mclapply except that it works on Windows and costs a little extra time up front.
3. **Makefile**: high-overhead, heavy-duty. For this one, Windows users need to download and install [`Rtools`](https://cran.r-project.org/bin/windows/Rtools/). For everyone else, just make sure [Make](https://www.gnu.org/software/make/) is installed.
- `make(my_plan, parallelism = "Makefile", jobs = 2)` creates a proper [Makefile](https://www.gnu.org/software/make/) to distribute the work over multiple independent R sessions. 
- `make(my_plan, parallelism = "Makefile", command = "make", args = "--jobs=4 --silent")` gives you finer control over the arguments and the program for the [Makefile](https://www.gnu.org/software/make/). For example, you could use [lsmake](https://www.ibm.com/support/knowledgecenter/en/SSETD4_9.1.2/lsf_command_ref/lsmake.1.html) instead of [Make](https://www.gnu.org/software/make/) itself.
- `make(my_plan, parallelism = "Makefile", jobs = 2, prepend = "SHELL=./shell.sh")` is similar, but it uses a helper file `shell.sh` to distribute the R sessions over different jobs/nodes on a cluster. Your `shell.sh` file should look something like the following, and you can use `shell_file()` to write an example.

```{r binbash, eval = FALSE}
#!/bin/bash
shift
echo "module load R; $*" | qsub -sync y -cwd -j y
```

You may need to replace `module load R` with a command to load a specific version of R. [SLURM](https://slurm.schedmd.com/) users can just point to `srun` and dispense with `shell.sh` altogether.

```{r cluster, eval = FALSE}
make(my_plan, parallelism = "Makefile", jobs = 4,
  prepend = "SHELL=srun")
```

For long projects, put your call to `make()` in an R script (say, `script.R`) and run it from the [Linux terminal](https://www.howtogeek.com/140679/beginner-geek-how-to-start-using-the-linux-terminal/).

```{r nohup, eval = FALSE}
nohup nice -19 R CMD BATCH script.R &
```

Even after you log out, a background process will keep running on the login node and submit new jobs at the appropriate time. Jobs are only submitted if the targets need to be (re)built.

## Important notes on Makefile-level parallelism

Makefile-level parallelism is only used for targets in your workflow plan data frame, not imports. To process imported objects and files, drake selects the best parallel backend for your system and uses the number of jobs you give to the `jobs` argument to `make()`. To use at most 2 jobs for imports and at most 4 jobs for targets, run

```r
make(..., parallelism = "Makefile", jobs = 2, args = "--jobs=4")
```

The [Makefile](http://kbroman.org/minimal_make/) generated by `make(plan, parallelism = "Makefile")` is not standalone. Do not run it outside of `drake::make()`. Drake uses dummy timestamp files to tell the [Makefile](http://kbroman.org/minimal_make/) which targets can be skipped and which need to be (re)built, so running `make` in the [terminal](https://www.howtogeek.com/140679/beginner-geek-how-to-start-using-the-linux-terminal/) will most likely give incorrect results.

```{r endofline, echo = F}
clean(destroy = TRUE) # Totally remove the hidden .drake/ cache.
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db")) # Clean up other files.
```
